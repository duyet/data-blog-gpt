import { Callout } from 'nextra-theme-docs'

<Callout>
Generated by GPT-3 at *Mon Apr 17 2023 00:46:16 GMT+0700 (Indochina Time)*
</Callout>

# Introduction to Data Lake

In the current scenario of data-driven businesses, it is quite essential to have a centralized repository of massive amounts of data. But storing and processing data at the enterprise level can be an expensive deal while dealing with relational databases. That's where Data Lakes come into the picture. A Data Lake is a centralized repository where you can store all structured and unstructured data at any scale. In this article, we will discuss Data Lake, its architecture, data ingestion, and data processing.

## What is Data Lake?

A Data Lake is a centralized repository that allows you to store large volumes of data in various formats, including structured, semi-structured, and unstructured data. It can store raw data from multiple sources, such as IoT devices, social media platforms, blogs, sensors, web pages, clickstreams, and others. The primary objective of Data Lakes is to store data from disparate sources in one place with minimal transformation to enable advanced analytics and machine learning with the help of powerful tools and resources.

## Data Lake Architecture

Data Lake architecture consists of several vital components that are responsible for storing, processing, and accessing massive amounts of data. Here are some essential components of the Data Lake architecture:

### Data Ingestion

Data Ingestion is the process of loading data from several sources into the Data Lake. The data ingestion process should be scalable, reliable, and automated. There are several tools available that can extract data from different sources, such as Sqoop, Kafka, Flume, NiFi, StreamSets, and others.

### Data Storage

Data Storage is the backbone of the Data Lake architecture. It should be capable of storing a wide range of data formats, including structured, semi-structured, and unstructured data. The most commonly used data storage technology for Data Lakes is Hadoop Distributed File System (HDFS).

### Data Processing

Data Processing is the crucial part of the Data Lake architecture, and it comprises several sub-components like:

- Data Transformation: It is the process of transforming raw data into a structured format, ready for processing. Tools like Apache Spark and Apache Hive can transform data from various sources, such as JSON, CSV, and Parquet into a format that can be analyzed.
- Data Quality: This component ensures that your data is consistent, accurate, and reliable. Data Quality tools like Apache Nifi, Talend, and Informatica are used to clean up the data and remove any errors.
- Data Governance: Data Governance is responsible for managing and controlling your Data Lake. It makes sure that your data is compliant with regulations, secure, and accessible only to authorized users.

### Data Access

Data Access is the process of accessing data stored in the Data Lake. There are several ways to access data, such as SQL engines like Apache Drill, Presto, and Apache Phoenix, and BI tools like Tableau, Power BI, and Qlikview.

## Data Ingestion to Data Lake

Data Ingestion is a crucial process in the Data Lake architecture, and it should be scalable, reliable, and automated. Some tools can extract data from several sources and load it into the Data Lake, such as Apache NiFi, StreamSets, and others. Here, we are going to use Apache NiFi to load data into our Data Lake.

### Apache NiFi

Apache NiFi is a powerful tool used for data routing, transformation, and system mediation. It helps to automate the flow of data between different systems in real-time. It provides a graphical interface to design data flows, which makes it easy for non-technical users to understand and develop data pipelines.

### Data Ingestion using Apache NiFi

Let's perform data ingestion using Apache NiFi. In this example, we will use a CSV file and load it into our Data Lake using Apache NiFi.

#### Pre-requisites

- Apache NiFi installed
- Data Lake running on Hadoop Distributed File System (HDFS)

#### Configure NiFi for Data Ingestion

Open your Apache NiFi UI and create a new flow.

Add the following processors to your flow:

- GetFile: Read files from the local file system
- PutHDFS: Write data to HDFS on your Data Lake

Connect the processors in the following way:

```
GetFile -> PutHDFS
```

Configure GetFile:

- Set the directory to pick up files from
- Set the file filter to pick up only CSV files

Configure PutHDFS:

- Set the directory to write the files to in HDFS
- Set the File Name property to generate file names dynamically

### Conclusion

Data Lakes are gaining popularity as a cost-effective way to store large volumes of structured and unstructured data. In this article, we have covered Data Lake architecture, data ingestion, and data processing. We also discussed a practical example of data ingestion using Apache NiFi. If you are considering building a Data Lake, this article will help you get started with the basic concepts.

Category: Data Lake